{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis — AQI Predictor\n",
    "**Location:** Hyderabad, Sindh, Pakistan  \n",
    "**Data Source:** Open-Meteo Air Quality + Weather APIs  \n",
    "**Target:** US AQI (US EPA Scale, 0–500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import sys, os\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from src.database import get_db_client\n",
    "import src.config as config\n",
    "\n",
    "sns.set_theme(style='darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 5)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = get_db_client()\n",
    "data = list(db[config.FEATURE_COLLECTION].find({}, {'_id': 0}).sort('time', 1))\n",
    "df = pd.DataFrame(data)\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df.set_index('time', inplace=True)\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "print(f'Records: {len(df)}')\n",
    "print(f'Date range: {df.index.min()} to {df.index.max()}')\n",
    "print(f'Duration: {(df.index.max() - df.index.min()).days} days')\n",
    "print(f'Columns: {list(df.columns)}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Overview & Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== Column Statistics ===')\n",
    "for col in df.select_dtypes(include=[np.number]).columns:\n",
    "    nulls = df[col].isnull().sum()\n",
    "    print(f'  {col:20s} | nulls={nulls:4d} | range=[{df[col].min():.1f}, {df[col].max():.1f}] | mean={df[col].mean():.1f}')\n",
    "\n",
    "# Check for time gaps\n",
    "time_diffs = df.index.to_series().diff().dropna()\n",
    "gaps = time_diffs[time_diffs > pd.Timedelta(hours=1)]\n",
    "print(f'\\nTime gaps > 1 hour: {len(gaps)}')\n",
    "if len(gaps) > 0:\n",
    "    for t, gap in gaps.items():\n",
    "        print(f'  {t}: {gap}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Target Variable (US AQI) Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df['us_aqi'].dropna(), bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(df['us_aqi'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"us_aqi\"].mean():.1f}')\n",
    "axes[0].axvline(df['us_aqi'].median(), color='orange', linestyle='--', label=f'Median: {df[\"us_aqi\"].median():.1f}')\n",
    "axes[0].set_xlabel('US AQI')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('AQI Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(df['us_aqi'].dropna(), vert=True)\n",
    "axes[1].set_ylabel('US AQI')\n",
    "axes[1].set_title('AQI Box Plot')\n",
    "\n",
    "# AQI categories\n",
    "cats = pd.cut(df['us_aqi'], bins=[0, 50, 100, 150, 200, 300, 500],\n",
    "              labels=['Good', 'Moderate', 'USG', 'Unhealthy', 'Very Unhealthy', 'Hazardous'])\n",
    "cat_counts = cats.value_counts().sort_index()\n",
    "colors = ['#00e400', '#ffff00', '#ff7e00', '#ff0000', '#8f3f97', '#7e0023']\n",
    "axes[2].bar(cat_counts.index, cat_counts.values, color=colors[:len(cat_counts)])\n",
    "axes[2].set_ylabel('Hours')\n",
    "axes[2].set_title('AQI Category Distribution')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nAQI Category Breakdown:')\n",
    "for cat, count in cat_counts.items():\n",
    "    print(f'  {cat:20s}: {count:5d} ({count/len(df)*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Time Series Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 6))\n",
    "ax.plot(df.index, df['us_aqi'], linewidth=0.5, alpha=0.8, color='steelblue')\n",
    "ax.axhline(50, color='green', linestyle='--', alpha=0.5, label='Good/Moderate boundary')\n",
    "ax.axhline(100, color='orange', linestyle='--', alpha=0.5, label='Moderate/USG boundary')\n",
    "ax.axhline(150, color='red', linestyle='--', alpha=0.5, label='USG/Unhealthy boundary')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('US AQI')\n",
    "ax.set_title('US AQI Time Series — Hyderabad, Sindh')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hourly and Day-of-Week Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Hourly pattern\n",
    "hourly = df.groupby(df.index.hour)['us_aqi'].mean()\n",
    "axes[0].bar(hourly.index, hourly.values, color='steelblue', alpha=0.8)\n",
    "axes[0].set_xlabel('Hour of Day')\n",
    "axes[0].set_ylabel('Mean AQI')\n",
    "axes[0].set_title('Average AQI by Hour of Day')\n",
    "axes[0].set_xticks(range(0, 24))\n",
    "\n",
    "peak_h = hourly.idxmax()\n",
    "low_h = hourly.idxmin()\n",
    "print(f'Peak AQI hour: {peak_h}:00 (AQI={hourly[peak_h]:.1f})')\n",
    "print(f'Lowest AQI hour: {low_h}:00 (AQI={hourly[low_h]:.1f})')\n",
    "print(f'Variation: {hourly.max() - hourly.min():.1f} AQI')\n",
    "\n",
    "# Day of week pattern\n",
    "dow = df.groupby(df.index.dayofweek)['us_aqi'].mean()\n",
    "dow_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "axes[1].bar(dow_names, dow.values, color='coral', alpha=0.8)\n",
    "axes[1].set_xlabel('Day of Week')\n",
    "axes[1].set_ylabel('Mean AQI')\n",
    "axes[1].set_title('Average AQI by Day of Week')\n",
    "\n",
    "print(f'\\nWeekday avg: {dow[:5].mean():.1f}')\n",
    "print(f'Weekend avg: {dow[5:].mean():.1f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Autocorrelation Analysis\n",
    "How well does past AQI predict future AQI?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [1, 3, 6, 12, 24, 48, 72]\n",
    "autocorrs = []\n",
    "\n",
    "aqi = df['us_aqi'].dropna()\n",
    "for lag in lags:\n",
    "    corr = aqi.corr(aqi.shift(lag))\n",
    "    autocorrs.append(corr)\n",
    "    print(f'  Lag {lag:3d}h: r = {corr:.3f}')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar([str(l) for l in lags], autocorrs, color='steelblue', alpha=0.8)\n",
    "ax.set_xlabel('Lag (hours)')\n",
    "ax.set_ylabel('Autocorrelation (r)')\n",
    "ax.set_title('AQI Autocorrelation at Different Lags')\n",
    "ax.axhline(0.7, color='red', linestyle='--', alpha=0.5, label='r=0.7 threshold')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nKey finding: AQI is highly persistent (r=0.74 at 24h lag).')\n",
    "print('This means lag features will be our strongest predictors.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Weather vs AQI Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_cols = ['temp', 'humidity', 'wind_speed', 'rain']\n",
    "pollutant_cols = ['pm10', 'no2', 'ozone']\n",
    "all_cols = weather_cols + pollutant_cols + ['us_aqi']\n",
    "\n",
    "corr_matrix = df[all_cols].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='RdBu_r', center=0, ax=ax,\n",
    "            square=True, linewidths=0.5)\n",
    "ax.set_title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nCorrelations with US AQI:')\n",
    "aqi_corr = corr_matrix['us_aqi'].drop('us_aqi').sort_values(key=abs, ascending=False)\n",
    "for col, corr in aqi_corr.items():\n",
    "    direction = 'Higher value → LOWER AQI' if corr < 0 else 'Higher value → HIGHER AQI'\n",
    "    print(f'  {col:15s}: r = {corr:+.3f}  ({direction})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. AQI by Temperature and Wind Speed\n",
    "The two strongest weather predictors from the correlation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Temperature bins\n",
    "df['temp_bin'] = pd.cut(df['temp'], bins=5)\n",
    "temp_aqi = df.groupby('temp_bin')['us_aqi'].mean()\n",
    "axes[0].bar(range(len(temp_aqi)), temp_aqi.values, color='tomato', alpha=0.8)\n",
    "axes[0].set_xticks(range(len(temp_aqi)))\n",
    "axes[0].set_xticklabels([f'{x.left:.0f}-{x.right:.0f}°C' for x in temp_aqi.index], rotation=45)\n",
    "axes[0].set_ylabel('Mean AQI')\n",
    "axes[0].set_title('Average AQI by Temperature Range')\n",
    "\n",
    "# Wind speed bins\n",
    "df['wind_bin'] = pd.cut(df['wind_speed'], bins=5)\n",
    "wind_aqi = df.groupby('wind_bin')['us_aqi'].mean()\n",
    "axes[1].bar(range(len(wind_aqi)), wind_aqi.values, color='skyblue', alpha=0.8)\n",
    "axes[1].set_xticks(range(len(wind_aqi)))\n",
    "axes[1].set_xticklabels([f'{x.left:.0f}-{x.right:.0f} km/h' for x in wind_aqi.index], rotation=45)\n",
    "axes[1].set_ylabel('Mean AQI')\n",
    "axes[1].set_title('Average AQI by Wind Speed Range')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nKey finding: Cold + calm = worst AQI (inversions).')\n",
    "print('Hot + windy = best AQI (mixing and dispersion).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. PM2.5 vs AQI Relationship\n",
    "US AQI in Hyderabad is driven almost entirely by PM2.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "valid = df[['pm2_5', 'us_aqi']].dropna()\n",
    "ax.scatter(valid['pm2_5'], valid['us_aqi'], alpha=0.1, s=5, color='steelblue')\n",
    "ax.set_xlabel('PM2.5 (µg/m³)')\n",
    "ax.set_ylabel('US AQI')\n",
    "ax.set_title(f'PM2.5 vs US AQI (r = {valid[\"pm2_5\"].corr(valid[\"us_aqi\"]):.3f})')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('PM2.5 is the dominant pollutant driving AQI in Hyderabad.')\n",
    "print('This is why we EXCLUDE pm2_5 from model features — it would be data leakage.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Design Rationale\n",
    "\n",
    "Based on the EDA findings, we designed **28 features** in 5 groups:\n",
    "\n",
    "| Group | Features | Rationale |\n",
    "|-------|----------|-----------|\n",
    "| **Weather** | temp, humidity, wind_speed, rain, weather_code | wind (r=-0.40), temp (r=-0.39) are strong predictors |\n",
    "| **Pollutants** | pm10, no2, ozone | pm10 (r=+0.45) strongest; available from forecast |\n",
    "| **Cyclical Time** | hour_sin/cos, dow_sin/cos, month_sin/cos | Captures diurnal, weekly, seasonal cycles |\n",
    "| **Explicit Time** | hour, is_night, is_morning_rush, etc. | Binary indicators for high-AQI periods |\n",
    "| **Lag/Rolling** | us_aqi_lag_24h, rolling_mean_24h, rolling_std_24h | lag_24h has r=0.739 — strongest predictor |\n",
    "| **Interactions** | wind_x_temp, humidity_x_temp, pm10_x_humidity | Capture non-linear relationships |\n",
    "\n",
    "### Excluded from features:\n",
    "- **us_aqi** — this is the target variable\n",
    "- **pm2_5** — directly derived from us_aqi (data leakage)\n",
    "- **city** — constant (always Hyderabad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Hyperparameter Tuning (RandomizedSearchCV)\n",
    "\n",
    "We tune all 3 models — **LightGBM**, **XGBoost**, and **RandomForest** — using `RandomizedSearchCV` with **TimeSeriesSplit** cross-validation to respect temporal ordering.\n",
    "\n",
    "**Why RandomizedSearchCV over GridSearchCV?**\n",
    "- Full grid search over 3 models with wide param spaces takes 1-2 hours\n",
    "- Randomized search with 20 iterations per model runs in ~5-10 minutes\n",
    "- This is critical because the training pipeline runs **daily at 1 AM PKT** via GitHub Actions (30-min timeout)\n",
    "- Research shows randomized search finds comparable results to grid search in far less time ([Bergstra & Bengio, 2012](https://jmlr.org/papers/v13/bergstra12a.html))\n",
    "\n",
    "The best parameters found here are **not hardcoded** into the training pipeline. Instead, the pipeline runs its own `RandomizedSearchCV` each day so that params adapt as data distribution shifts over time."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Prepare training data (same pipeline as train_forecast_model.py) ---\n",
    "TARGET_COL = 'us_aqi'\n",
    "\n",
    "df_tune = df.copy()\n",
    "\n",
    "# Engineer features (same as training pipeline)\n",
    "df_tune['hour_sin'] = np.sin(2 * np.pi * df_tune.index.hour / 24)\n",
    "df_tune['hour_cos'] = np.cos(2 * np.pi * df_tune.index.hour / 24)\n",
    "df_tune['dow_sin'] = np.sin(2 * np.pi * df_tune.index.dayofweek / 7)\n",
    "df_tune['dow_cos'] = np.cos(2 * np.pi * df_tune.index.dayofweek / 7)\n",
    "df_tune['month_sin'] = np.sin(2 * np.pi * df_tune.index.month / 12)\n",
    "df_tune['month_cos'] = np.cos(2 * np.pi * df_tune.index.month / 12)\n",
    "df_tune['hour'] = df_tune.index.hour\n",
    "df_tune['is_night'] = ((df_tune.index.hour >= 20) | (df_tune.index.hour <= 5)).astype(int)\n",
    "df_tune['is_morning_rush'] = ((df_tune.index.hour >= 6) & (df_tune.index.hour <= 10)).astype(int)\n",
    "df_tune['is_afternoon'] = ((df_tune.index.hour >= 11) & (df_tune.index.hour <= 17)).astype(int)\n",
    "df_tune['is_evening_rush'] = ((df_tune.index.hour >= 17) & (df_tune.index.hour <= 20)).astype(int)\n",
    "df_tune['is_weekend'] = (df_tune.index.dayofweek >= 5).astype(int)\n",
    "df_tune[f'{TARGET_COL}_lag_24h'] = df_tune[TARGET_COL].shift(24).bfill().ffill()\n",
    "df_tune[f'{TARGET_COL}_rolling_mean_24h'] = df_tune[TARGET_COL].rolling(24, min_periods=1).mean().bfill()\n",
    "df_tune[f'{TARGET_COL}_rolling_std_24h'] = df_tune[TARGET_COL].rolling(24, min_periods=1).std().fillna(0)\n",
    "df_tune['wind_x_temp'] = df_tune['wind_speed'] * df_tune['temp']\n",
    "df_tune['humidity_x_temp'] = df_tune['humidity'] * df_tune['temp']\n",
    "df_tune['wind_x_humidity'] = df_tune['wind_speed'] * df_tune['humidity']\n",
    "df_tune['pm10_x_humidity'] = df_tune['pm10'] * df_tune['humidity']\n",
    "df_tune['hour_x_ozone'] = df_tune['hour'] * df_tune['ozone']\n",
    "df_tune = df_tune.dropna()\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    'temp', 'humidity', 'wind_speed', 'rain', 'weather_code',\n",
    "    'pm10', 'no2', 'ozone',\n",
    "    'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos', 'month_sin', 'month_cos',\n",
    "    'hour', 'is_night', 'is_morning_rush', 'is_afternoon', 'is_evening_rush', 'is_weekend',\n",
    "    f'{TARGET_COL}_lag_24h', f'{TARGET_COL}_rolling_mean_24h', f'{TARGET_COL}_rolling_std_24h',\n",
    "    'wind_x_temp', 'humidity_x_temp', 'wind_x_humidity', 'pm10_x_humidity', 'hour_x_ozone',\n",
    "]\n",
    "\n",
    "X = df_tune[[c for c in FEATURE_COLS if c in df_tune.columns]]\n",
    "y = df_tune[TARGET_COL]\n",
    "\n",
    "# Chronological split\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "print(f'Training samples: {len(X_train)} | Test samples: {len(X_test)}')\n",
    "print(f'Features: {len(X.columns)}')\n",
    "\n",
    "# --- Define search spaces ---\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "param_spaces = {\n",
    "    'LightGBM': {\n",
    "        'estimator': lgb.LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1),\n",
    "        'params': {\n",
    "            'n_estimators': [200, 300, 500, 700],\n",
    "            'max_depth': [5, 8, 12, -1],\n",
    "            'learning_rate': [0.01, 0.03, 0.05, 0.1],\n",
    "            'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "            'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "            'min_child_samples': [10, 20, 30, 50],\n",
    "            'reg_alpha': [0.0, 0.01, 0.1, 1.0],\n",
    "            'reg_lambda': [0.0, 0.01, 0.1, 1.0],\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'estimator': xgb.XGBRegressor(random_state=42, n_jobs=-1, verbosity=0),\n",
    "        'params': {\n",
    "            'n_estimators': [200, 300, 500, 700],\n",
    "            'max_depth': [5, 8, 12, 15],\n",
    "            'learning_rate': [0.01, 0.03, 0.05, 0.1],\n",
    "            'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "            'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "            'reg_alpha': [0.0, 0.01, 0.1, 1.0],\n",
    "            'reg_lambda': [0.0, 0.01, 0.1, 1.0],\n",
    "        }\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'estimator': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        'params': {\n",
    "            'n_estimators': [200, 300, 500],\n",
    "            'max_depth': [10, 15, 20, None],\n",
    "            'min_samples_split': [5, 10, 20],\n",
    "            'min_samples_leaf': [3, 5, 10],\n",
    "            'max_features': ['sqrt', 0.5, 0.8],\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Run RandomizedSearchCV for each model ---\n",
    "tuning_results = {}\n",
    "\n",
    "for name, spec in param_spaces.items():\n",
    "    print(f'\\n{\"=\"*50}')\n",
    "    print(f'  Tuning {name} (20 iterations, 3-fold TimeSeriesSplit)')\n",
    "    print(f'{\"=\"*50}')\n",
    "    \n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=spec['estimator'],\n",
    "        param_distributions=spec['params'],\n",
    "        n_iter=20,\n",
    "        cv=tscv,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on held-out test set\n",
    "    best_model = search.best_estimator_\n",
    "    preds = best_model.predict(X_test)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    \n",
    "    tuning_results[name] = {\n",
    "        'best_params': search.best_params_,\n",
    "        'cv_mae': -search.best_score_,\n",
    "        'test_r2': r2,\n",
    "        'test_mae': mae,\n",
    "        'test_rmse': rmse,\n",
    "        'model': best_model\n",
    "    }\n",
    "    \n",
    "    print(f'  Best CV MAE: {-search.best_score_:.2f}')\n",
    "    print(f'  Test R\\u00b2:     {r2:.4f}')\n",
    "    print(f'  Test MAE:    {mae:.2f}')\n",
    "    print(f'  Test RMSE:   {rmse:.2f}')\n",
    "    print(f'  Best params: {search.best_params_}')\n",
    "\n",
    "# --- Summary comparison ---\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'  TUNING RESULTS SUMMARY')\n",
    "print(f'{\"=\"*60}')\n",
    "print(f'  {\"Model\":<15} {\"CV MAE\":>8} {\"Test R\\u00b2\":>8} {\"Test MAE\":>8} {\"RMSE\":>8}')\n",
    "print(f'  {\"-\"*55}')\n",
    "\n",
    "best_name = min(tuning_results, key=lambda k: tuning_results[k]['test_mae'])\n",
    "for name, r in tuning_results.items():\n",
    "    star = ' \\u25c0 BEST' if name == best_name else ''\n",
    "    print(f'  {name:<15} {r[\"cv_mae\"]:8.2f} {r[\"test_r2\"]:8.4f} {r[\"test_mae\"]:8.2f} {r[\"test_rmse\"]:8.2f}{star}')\n",
    "\n",
    "print(f'\\n  Winner: {best_name} (Test MAE = {tuning_results[best_name][\"test_mae\"]:.2f})')\n",
    "print(f'\\n  Note: The training pipeline runs its own RandomizedSearchCV daily')\n",
    "print(f'  so parameters adapt as data distribution changes over time.')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Visualize tuning results ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Model comparison bar chart\n",
    "models_list = list(tuning_results.keys())\n",
    "maes = [tuning_results[m]['test_mae'] for m in models_list]\n",
    "r2s = [tuning_results[m]['test_r2'] for m in models_list]\n",
    "colors = ['#2ecc71' if m == best_name else '#3498db' for m in models_list]\n",
    "\n",
    "axes[0].bar(models_list, maes, color=colors, alpha=0.8, edgecolor='black')\n",
    "axes[0].set_ylabel('Test MAE (lower is better)')\n",
    "axes[0].set_title('Model Comparison \\u2014 MAE')\n",
    "for i, v in enumerate(maes):\n",
    "    axes[0].text(i, v + 0.2, f'{v:.2f}', ha='center', fontweight='bold')\n",
    "\n",
    "axes[1].bar(models_list, r2s, color=colors, alpha=0.8, edgecolor='black')\n",
    "axes[1].set_ylabel('Test R\\u00b2 (higher is better)')\n",
    "axes[1].set_title('Model Comparison \\u2014 R\\u00b2')\n",
    "for i, v in enumerate(r2s):\n",
    "    axes[1].text(i, v + 0.002, f'{v:.4f}', ha='center', fontweight='bold')\n",
    "\n",
    "# 2. Best model's feature importance\n",
    "best_tuned = tuning_results[best_name]['model']\n",
    "if hasattr(best_tuned, 'feature_importances_'):\n",
    "    fi = sorted(zip(X.columns, best_tuned.feature_importances_), key=lambda x: x[1], reverse=True)[:15]\n",
    "    names, imps = zip(*fi)\n",
    "    axes[2].barh(range(len(names)), imps, color='steelblue', alpha=0.8)\n",
    "    axes[2].set_yticks(range(len(names)))\n",
    "    axes[2].set_yticklabels(names)\n",
    "    axes[2].invert_yaxis()\n",
    "    axes[2].set_xlabel('Feature Importance')\n",
    "    axes[2].set_title(f'{best_name} \\u2014 Top 15 Features')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print best params for each model\n",
    "for name, r in tuning_results.items():\n",
    "    print(f'\\n{name} best parameters:')\n",
    "    for param, value in sorted(r['best_params'].items()):\n",
    "        print(f'  {param}: {value}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Open-Meteo Data Delay Analysis\n",
    "Understanding the gap between real-time and available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Query Open-Meteo with forecast_days=0 to get ONLY real (non-forecast) data\n",
    "url = 'https://air-quality-api.open-meteo.com/v1/air-quality'\n",
    "params = {\n",
    "    'latitude': config.LAT, 'longitude': config.LON,\n",
    "    'hourly': ['us_aqi'],\n",
    "    'past_days': 3, 'forecast_days': 0,\n",
    "    'timezone': 'Asia/Karachi'\n",
    "}\n",
    "\n",
    "resp = requests.get(url, params=params, timeout=30).json()\n",
    "times = resp['hourly']['time']\n",
    "aqis = resp['hourly']['us_aqi']\n",
    "\n",
    "# Find last non-null\n",
    "for i in range(len(aqis)-1, -1, -1):\n",
    "    if aqis[i] is not None:\n",
    "        from datetime import datetime, timedelta, timezone as tz\n",
    "        PKT = tz(timedelta(hours=5))\n",
    "        now = datetime.now(PKT).replace(tzinfo=None)\n",
    "        last = datetime.fromisoformat(times[i])\n",
    "        delay = (now - last).total_seconds() / 3600\n",
    "        print(f'Current time (PKT): {now.strftime(\"%Y-%m-%d %H:%M\")}')\n",
    "        print(f'Last real AQI: {aqis[i]} at {times[i]}')\n",
    "        print(f'Delay: {delay:.1f} hours')\n",
    "        print()\n",
    "        print('The air quality data comes from CAMS (Copernicus) which updates daily.')\n",
    "        print('Weather data (temp, wind) updates every 1-6 hours.')\n",
    "        print('When using forecast_days>=1, today\\'s hours are filled with CAMS model predictions.')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Key Findings\n",
    "\n",
    "1. **AQI is predominantly Moderate** (51-100) with occasional USG days\n",
    "2. **Strong autocorrelation** (r=0.74 at 24h lag) — yesterday's AQI is the best predictor\n",
    "3. **Wind speed** (r=-0.40) and **temperature** (r=-0.39) are the strongest weather predictors\n",
    "4. **PM10** (r=+0.45) is the strongest pollutant predictor (physically related to PM2.5)\n",
    "5. **Mild diurnal pattern** (~3.4 AQI variation) — evening peak, afternoon low\n",
    "6. **No weekday/weekend difference** — AQI driven by weather, not local traffic\n",
    "7. **Cold + calm = worst AQI** (temperature inversions trap pollutants)\n",
    "8. **Data is continuous** — no time gaps in the 189-day dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}